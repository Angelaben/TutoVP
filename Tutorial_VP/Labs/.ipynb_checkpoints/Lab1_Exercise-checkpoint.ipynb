{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import ENV.Stationary_Bandit_ENV as S_bandit_env\n",
    "from ENV.Logger import Logger\n",
    "import seaborn as sns\n",
    "import math\n",
    "np.random.seed(42) # For reproductibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The reinforcement :\n",
    "\n",
    "The main difference between reinforcement learning and other types of learning is that it uses training that evaluates an action without instruction on whether it was a correct to take or not.\n",
    "Thus one basic and fundamental strategy for a reinforcement learning agent is to explore its environment the best it can and learn from it as much information as possible with only a few experiments.\n",
    "One can easily imagine how complex it would be to revisit a large amount of times the same states in a very large universe of possibilities.\n",
    "\n",
    "Thus the main difficulty for reinforcement learning algorithm is to find the correct tradeoff between exploration and exploitation, it is a vast domain which we will only present superficially\n",
    "\n",
    "To facilitate your introduction to this field, we will begin with a simplification of the problem called Multi Armed bandits. It is a single stationary state composed of K bandits which you can interact one at a time. \n",
    "Your purpose will be to find the optimal arm to trigger to maximize your reward over time.\n",
    "\n",
    "#### A bandit  : As indicated by its name, it is a lever which awards you for pulling him based on a Normal law which mean is determined by a $N(0, 1)$ and its variance is 1\n",
    "#### Exploitation : The phase where the agent exploits its policy to determine which lever to pull\n",
    "#### Exploration : The phase where the agent experiments at (more or less) random to improve its knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 1 : Stationary Environment - Multi Armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon-greedy$ action value ( sample average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first exercise you will have to fill in the blanks to :\n",
    "- Implement the $\\epsilon$-greedy strategy -> Constant or linear\n",
    "- Implement the Qtable update : $$Q(a) = Q(a) + \\frac{1}{n_{observed}(a)} \\times (reward - Q(a))$$ with $n_{observed}$ the number of times 'a' has been chosen until now\n",
    "\n",
    "Remember that the algorithm is :\n",
    "```python\n",
    "S = env.reset()\n",
    "\n",
    "a = agent.choose_action(S)\n",
    "\n",
    "reward = env.step(a)\n",
    "\n",
    "agent.update_parameters()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_solver() :\n",
    "\n",
    "    def __init__(self, env, timestep = 1000) :\n",
    "        self.env = env\n",
    "        self.nb_bandits = env.get_nb_bandit\n",
    "        self.Q_table = # TODO\n",
    "        self.action_count = # TODO\n",
    "        self.timestep = timestep\n",
    "        # Parameters initialisation\n",
    "        self.epsilon = 1.\n",
    "        self.print_delay = 0.1\n",
    "        self.epsilon_origin = 1.\n",
    "        self.epsilon_min = 0.02\n",
    "        self.epsilon_decay = timestep * 0.25\n",
    "        # Logger\n",
    "        self.logger = Logger()\n",
    "\n",
    "    def act(self, act_epsilon_greedy) :\n",
    "        if act_epsilon_greedy :\n",
    "            # Act randomly\n",
    "            pass\n",
    "        else :\n",
    "            # Act greedy\n",
    "            pass\n",
    "\n",
    "    def updateQtable(self, action, reward, action_count) :\n",
    "        # TODO \n",
    "        pass\n",
    "    \n",
    "    def run(self, force_epsilon = None) :\n",
    "        self.boxplotter = [[0]] * self.nb_bandits\n",
    "        if force_epsilon is not None :\n",
    "            self.epsilon = force_epsilon\n",
    "        env.reset()\n",
    "        cumul_reward = 0\n",
    "\n",
    "        for iteration in trange(1, self.timestep) :\n",
    "            action = # TODO\n",
    "            reward = env.step(action)\n",
    "            cumul_reward += reward\n",
    "            self.boxplotter[action].append(reward)\n",
    "            ## UPDATE Q TABLE\n",
    "            if force_epsilon is None:\n",
    "                # Decay epsilon\n",
    "                pass\n",
    "            self.logger.epsilon_log(self.epsilon)\n",
    "            self.logger.reward_log(reward)\n",
    "            if iteration % (self.timestep * self.print_delay) == 0 :\n",
    "                self.logger.mean_reward_log(cumul_reward / (self.timestep * self.print_delay))\n",
    "                self.logger.plot_log(2)\n",
    "                cumul_reward = 0\n",
    "                plt.boxplot(solver.boxplotter)\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Solution/Lab1_solution.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the environment \n",
    "env = S_bandit_env.Stationary_Bandit()\n",
    "max_arm, max_mean = env.get_max_bandit()\n",
    "print(\"Maximum arm is \", max_arm, \" with mean \" ,max_mean)\n",
    "env.list_mean_bandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = Q_solver(env, timestep=100)\n",
    "solver.run(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(solver.Q_table.T, cmap = \"coolwarm\")\n",
    "solver.Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(solver.boxplotter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another strategy for  stationary environment : UCB, Upper-Confidence-Bound Action Selection\n",
    "\n",
    "Exploration is needed because there is always uncertainty about the accuracy of the Q(s,a) estimation. The $\\epsilon$-greedy force the algorithm to choose indiscriminately between all the available actions.\n",
    "UCB is a strategy to improve the exploration during the Q value iteration : Select among the actions to be tried the action which potential improvment it represent for the agent's knowledge of its environment is the greatest. In another word select the action which uncertainty and improvement's potential estimate is the higher, instead of selecting at random the actions.\n",
    "\n",
    "The new act method will be : \n",
    "\n",
    "$$A_T=argmax_A Q_t(a) + c\\sqrt{\\frac{ln(t)}{N_t(a)}}$$\n",
    "\n",
    "with ln(t) the natural logarithm of t, $N_T(a)$ the number of times \"a\" has been selected until now and c a constant which represents the degree of desired exploration.\n",
    "\n",
    "If $N_t(a) = 0$ then a is considered to be maximizing the action.\n",
    "\n",
    "\"The idea of this upper confidence bound (UCB) action selection is that the square-root\n",
    "term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity\n",
    "being max’ed over is thus a sort of upper bound on the possible true value of action a, with\n",
    "c determining the confidence level. Each time a is selected the uncertainty is presumably\n",
    "reduced: Nt(a) increments, and, as it appears in the denominator, the uncertainty term\n",
    "decreases. On the other hand, each time an action other than a is selected, t increases but\n",
    "Nt(a) does not; because t appears in the numerator, the uncertainty estimate increases.\n",
    "The use of the natural logarithm means that the increases get smaller over time, but are\n",
    "unbounded; all actions will eventually be selected, but actions with lower value estimates,\n",
    "or that have already been selected frequently, will be selected with decreasing frequency\n",
    "over time.\" Sutton(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_solver_UCB(Q_solver) :\n",
    "\n",
    "    def __init__(self, env, timestep = 1000) :\n",
    "        Q_solver.__init__(self, env, timestep)\n",
    "        self.c = 0.75\n",
    "    def act(self, time, act_counter):\n",
    "        pass\n",
    "    # TODO\n",
    "    \n",
    "    def run(self, force_epsilon = None) :\n",
    "        self.boxplotter = [[0]] * self.nb_bandits\n",
    "        env.reset()\n",
    "        cumul_reward = 0\n",
    "\n",
    "        for iteration in trange(1, self.timestep) :\n",
    "            action = # TODO\n",
    "            # Action count\n",
    "            reward = env.step(action)\n",
    "            cumul_reward += reward\n",
    "            self.boxplotter[action].append(reward)\n",
    "            # Update Q table\n",
    "            self.logger.reward_log(reward)\n",
    "            if iteration % (self.timestep * self.print_delay) == 0 :\n",
    "                self.logger.mean_reward_log(cumul_reward / (self.timestep * self.print_delay))\n",
    "                self.logger.plot_log(5)\n",
    "                cumul_reward = 0\n",
    "                plt.boxplot(solver.boxplotter)\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Solution/Lab1_exo1_UCB.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the environment \n",
    "env = S_bandit_env.Stationary_Bandit()\n",
    "max_arm, max_mean = env.get_max_bandit()\n",
    "print(\"Maximum arm is \", max_arm, \" with mean \" ,max_mean)\n",
    "env.list_mean_bandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "solver = Q_solver_UCB(env, timestep = 10000)\n",
    "solver.run(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(solver.Q_table.T, cmap = \"coolwarm\")\n",
    "solver.Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(solver.boxplotter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poor results ? possibly - Optimistic initialisation\n",
    "\n",
    "All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, Q1(a).  The initial estimates become a parameter you have to modify and select which may be an inconvenient, but it can also be a benefits : it is a way to gives knowledge to the algorithm, about for example the expectec reward.\n",
    "In this case the initial action value can be used as a simple way to encourage exploration.\n",
    "Here, let's use a Q table which initial values are 5 for every arm, with expected mean reward of approximately 1 for each arm in reality we largely overestimate our expected reward.\n",
    "Whichever actions are initially selected, the reward is less than the pre computed Q table's value; the agent will thus switches to other actions, being “disappointed” with the rewards it is receiving with this choice. The result is that all actions are tried several times before the value estimates converge. \n",
    "\n",
    "WARNING : results may or may not be better, don't forget a lot of random is involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_solver_UCB(Q_solver) :\n",
    "\n",
    "    def __init__(self, env, timestep = 1000) :\n",
    "        Q_solver.__init__(self, env, timestep)\n",
    "        self.c = 0.75\n",
    "        self.Q_table = np.ones((self.nb_bandits, 1)) * 2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_vanilla_UCB = solver.logger.cumulative_mean # To compare later both initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the environment \n",
    "env = S_bandit_env.Stationary_Bandit()\n",
    "max_arm, max_mean = env.get_max_bandit()\n",
    "print(\"Maximum arm is \", max_arm, \" with mean \" ,max_mean)\n",
    "env.list_mean_bandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = Q_solver_UCB(env, timestep = 10000)\n",
    "solver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(solver.Q_table.T, cmap = \"coolwarm\")\n",
    "solver.Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_to = solver.logger.cumulative_mean \n",
    "plt.plot(compare_to, label = \"Optimistic\")\n",
    "plt.plot(memory_vanilla_UCB, label = \"Vanilla\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another strategy for non stationary environment : Policy based gradient bandit algorithm\n",
    "So far we have experimented methods which consist of estimating how valuable an action is by approximating the action-value function. This is a really good strategy which is often used, but there is another method which relies on finding a good policy to follow, building it step by step by a die and retry strategy.\n",
    "This method called policy based gradient relies on building a preference vector (the larger the preference for an action, the more often it will be sampled).\n",
    "To put it simply, instead of trying to modelize the value of each state of your environment, you will try to find which action is the most likely to be interesting in regards of your current state (you don't know the value of the state, you estimate it, in comparison with action-value method where you consider your value function as the real value of the state).\n",
    "\n",
    "In the preference formula you can observe the following elements :\n",
    "- $R_t$ the last obtained reward\n",
    "- $\\bar{R}_t$ the average reward, sort of baseline which you compare the reward to, if your new reward is higher you add a positive feedback, or a negative if your reward is lower than the average.\n",
    "- $\\alpha$ a learning rate\n",
    "- $\\pi(A_t)_t$ the probability of taking action \"A_t\" at time \"t\" following your policy\n",
    "/!\\ NB : Watch out, this algorithm converge extremely fast, don't use too large timestep\n",
    "\n",
    "\n",
    "The algorithm will be the following \n",
    "```python\n",
    "preference_vector = initialise preference_vector\n",
    "begin loop:\n",
    "    policy = compute_policy(preference_vector)\n",
    "    action = act(policy)\n",
    "    reward = env.step(action)\n",
    "    update_preferences(reward)\n",
    "end loop\n",
    "```\n",
    "\n",
    "#### Formula \n",
    "- Policy :\n",
    "$$\\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^ke^{H_t(b)}}$$\n",
    "- Average reward :\n",
    "$$Q_{n+1} = Q_n + \\frac{1}{n}(R_n - Q_n)$$\n",
    "- Preferences :\n",
    "$$H_{t+1}(A_t) = H_t(A_t) + \\alpha(R_t - \\bar{R}_t)(1 - \\pi_t(A_t))\\ if\\ A_t = a$$\n",
    "$$H_{t+1}(a) = H_t(a) - \\alpha(R_t - \\bar{R}_t)\\pi_t(A_t)\\ \\forall a \\neq A_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_solver_policy(Q_solver) :\n",
    "\n",
    "    def __init__(self, env, timestep = 1000) :\n",
    "        Q_solver.__init__(self, env, timestep)\n",
    "        self.alpha = 0.1\n",
    "\n",
    "    def act(self, policy) :\n",
    "       # TODO\n",
    "        pass\n",
    "\n",
    "    def update_preferences(self, preferences, action, reward, mean_reward, policy) :\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def compute_policy(self, preferences) :\n",
    "        #TODO\n",
    "        pass\n",
    " # Ignore the epsilon \n",
    "    def run(self, force_epsilon = None) :\n",
    "\n",
    "        env.reset()\n",
    "        cumul_reward = 0\n",
    "        preferences = np.zeros((self.nb_bandits, 1))\n",
    "        average_reward = 0.0\n",
    "        for iteration in trange(1, self.timestep) :\n",
    "            policy = self.compute_policy(preferences)\n",
    "            action = self.act(policy)\n",
    "            reward = env.step(action)\n",
    "            self.logger.reward_log(reward)\n",
    "            cumul_reward += reward\n",
    "            average_reward += (reward - average_reward) / iteration\n",
    "            preferences = self.update_preferences(preferences, action, reward, average_reward, policy)\n",
    "            if iteration % (self.timestep * self.print_delay) == 0 :\n",
    "                self.logger.plot_mean_reward()\n",
    "                cumul_reward = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Solution/Lab1_exo1_Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the environment \n",
    "env = S_bandit_env.Stationary_Bandit()\n",
    "max_arm, max_mean = env.get_max_bandit()\n",
    "print(\"Maximum arm is \", max_arm, \" with mean \" ,max_mean)\n",
    "env.list_mean_bandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "solver = Q_solver_policy(env, timestep=400)\n",
    "solver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(solver.preferences.T, cmap=\"coolwarm\")\n",
    "solver.preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2 : Non stationary environment - Multi Armed Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ENV.Non_Stationary_Bandit_ENV as NS_bandit_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The non stationarity :\n",
    "\n",
    "As you can easily imagine the last experiment was highly unlikely in the real world : Imagine for a company like vente-privee if its users' tastes never changed.\n",
    "This is why another difficulty arises for reinforcement learning algorithm, and this is one of the main problem it has to tackle : the non stationarity in the environment.\n",
    "It is necessary for an agent to explore as thoroughly as possible its environment, to be as precise as possible in its exploration/exploitation trade-off, but also to remain plastic enough to adapt to a possible brutal variation in the environment (or even a subtle one).\n",
    "And in this new exercise this will be what you will have to experiment.\n",
    "\n",
    "#### The new environment :\n",
    "\n",
    "You are still experimenting over a K-armed bandits problems, with mean and sigma as precedently defined, but we add a little noise or variation in the mean. Each time an arm is selected, all arms' means will be slightly shifted (up or down, changing their \"interest\"). And your agent will have to adapt of it, or not.\n",
    "\n",
    "You will observe that the reward might remain high, or even significantly larger than the last problem if your \"favorite\" arm improve as time passes. But how can you be sure that it remains the best ? \n",
    "\n",
    "How would you modify the precedent algorithm to measure if you still follow the good policy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment last solver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.list_mean_bandit() # Observe the best arm at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = NS_bandit_env.Non_Stationary_Bandit()\n",
    "solver = Q_solver_policy(env)\n",
    "solver.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.list_mean_bandit() # observe at the end\n",
    "sns.heatmap(solver.preferences.T, cmap=\"coolwarm\")\n",
    "solver.preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to slightly tackle this problem is to introduce a little $\\alpha$ which purpose will be to transform the Q update rule into a weighted average of past rewards.\n",
    "To stay simple just remind that $\\alpha$ has to respect the following condition : $\\alpha \\in\\ ]0;1]$\n",
    "It is a strategy to somehow gives more weight to recent rewards than the long past rewards :\n",
    "\n",
    "$$\n",
    "Q_{n+1} = Q_n + \\alpha \\times (R_n - Q_n)\\\\\n",
    "        = \\alpha R_n + (1 - \\alpha)Q_n\\\\\n",
    "        = ...\\\\\n",
    "        = (1 - \\alpha)^nQ_1 + \\sum_{i=1}^n \\alpha(1-\\alpha)^{n-i}R_i\n",
    "        $$\n",
    "        \n",
    "Other strategies exist but for clarity and simplicy reasons we won't implements them here.\n",
    "\n",
    "Your algorithm has to be the following :\n",
    "```python\n",
    "updateQtable(state, alpha):\n",
    "\n",
    "    Q(a) = Q(a) + alpha * (reward - Q(a))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_solver_NS(Q_solver):\n",
    "    def __init__(self, env, timestep = 1000):\n",
    "        Q_solver.__init__(self, env, timestep, epsilon_min)\n",
    "        self.alpha = 0.1\n",
    "    \n",
    "    def updateQtable(self, action, reward, action_count):\n",
    "        pass # TODO\n",
    "    \n",
    "    def run(self, force_epsilon = None) :\n",
    "        if force_epsilon is not None :\n",
    "            self.epsilon = force_epsilon\n",
    "        env.reset()\n",
    "        cumul_reward = 0\n",
    "\n",
    "        for iteration in trange(1, self.timestep) :\n",
    "            action = # TODO\n",
    "            # Action count\n",
    "            reward = env.step(action)\n",
    "            cumul_reward += reward\n",
    "            # update Q table\n",
    "            self.logger.epsilon_log(self.epsilon)\n",
    "            self.logger.reward_log(reward)\n",
    "            if iteration % (self.timestep * self.print_delay) == 0 :\n",
    "                self.logger.mean_reward_log(cumul_reward / (self.timestep * self.print_delay))\n",
    "                self.logger.plot_log(3, 3)\n",
    "                cumul_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Solution/Lab1_exo2_solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NS_bandit_env.Non_Stationary_Bandit()\n",
    "solver = Q_solver_NS(env)\n",
    "solver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(solver.Q_table.T, cmap=\"coolwarm\")\n",
    "solver.Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3 : Contextual Environment - Multi Armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we have only considered non associative tasks, tasks in which there is no need to associate actions over different situations. In those tasks your main purpose was to find an optimal action no matter the situation, or to track the evolution of the best action to change your policy over time.\n",
    "\n",
    "#### Contextual bandit\n",
    "\n",
    "In this new simplified problem, we will suppose that on each arm there is a little light which colors is either red or green. Depending on the color of the light, the distribution of each arm's probability is different (slightly, or not...). The purpose of this agent will be to adapt its policy depending on this simple state (a boolean here).\n",
    "\n",
    "We call this type of problem \"associative\" because you have to experiment your trial-and-error strategy and in the mean time associate thoe actions with the situations you are in, and thus extrapolate the optimal strategy depending on the state of the game.\n",
    "\n",
    "\n",
    "Your algorithm has to be :\n",
    "```python\n",
    "S, context = env.reset()\n",
    "\n",
    "a = agent.choose_action(S, context)\n",
    "\n",
    "reward, context = env.step(a)\n",
    "\n",
    "agent.update_parameters()\n",
    "\n",
    "updateQtable(state, context):\n",
    "    Q(s,a) = Q(s,a) + __ * (reward - Q(s,a))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ENV.ContextualBandit as C_Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Q_solver_contextual(Q_solver) :\n",
    "\n",
    "    def __init__(self, env, timestep = 1000) :\n",
    "        Q_solver.__init__(self, env, timestep)\n",
    "        self.nb_context = env.get_nb_context\n",
    "        self.Q_table = # TODO\n",
    "        self.action_count = # TODO\n",
    "        self.timestep = timestep\n",
    "        # Parameters initialisation\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_origin = 1.\n",
    "        self.epsilon_decay = timestep * 0.25\n",
    "        self.epsilon_min = 0.02\n",
    "        self.print_delay = 0.1\n",
    "        \n",
    "    def act(self, act_epsilon_greedy, context) :\n",
    "        #TODO\n",
    "        pass\n",
    "    def updateQtable(self, action, reward, action_count, context) :\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def run(self, force_epsilon = None) :\n",
    "        context = env.reset()\n",
    "        cumul_reward = 0\n",
    "        for iteration in trange(1, self.timestep) :\n",
    "            action = # TODO\n",
    "            # Count action\n",
    "            reward, context = env.step(action)\n",
    "            # Update Q table\n",
    "            # Update epsilon with linear decay\n",
    "            self.epsilon = max(self.epsilon - self.epsilon_origin / self.epsilon_decay, self.epsilon_min)\n",
    "            if force_epsilon is not None :\n",
    "                self.epsilon = force_epsilon\n",
    "            cumul_reward += reward\n",
    "            self.logger.epsilon_log(self.epsilon)\n",
    "            self.logger.reward_log(reward)\n",
    "            if iteration % (self.timestep * self.print_delay) == 0 :\n",
    "                self.logger.mean_reward_log(cumul_reward / (self.timestep * self.print_delay))\n",
    "                self.logger.plot_log(10, 10)\n",
    "                cumul_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Solution/Lab1_exo3_solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = C_Bandit.Contextual_bandit(n_bandit=10)\n",
    "solver = Q_solver_contextual(env, timestep=1000)\n",
    "solver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(solver.Q_table.T, cmap=\"coolwarm\")\n",
    "solver.Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 4 : Cartpole and DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Dense, Input, Add, concatenate, RepeatVector, Flatten, Lambda, Conv2D, MaxPooling2D, UpSampling2D, Reshape\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q network \n",
    "\n",
    "In this chapter we will introduce you to the nearly state of the art technique in reinforcement learning.\n",
    "As indicated by its name DQN consist of a deep learning model trying to predict the Qtable and its evolution through time.\n",
    "To perform this analysis it requires a tuple (State, action, reward, Next_state) to compare its prediction using the state, and the ground truth (action, reward, next_state).\n",
    "Multiple improvement exist for such algorithm which we won't detail here but feel free to ask questions about it.\n",
    "\n",
    "Your algorithm will be as follow :\n",
    "```python\n",
    "state = env.reset()\n",
    "action = agent.act(state) # Following epsilon greedy\n",
    "reward, next_state, endofGame = agent.step(action)\n",
    "memorize(state, action, reward, next_state, done)\n",
    "updateModel(memory)\n",
    "\n",
    "\n",
    "def updateModel(memory):\n",
    "    samples = extract_batch_from_memory(memory, batch_size)\n",
    "    for sample in samples:\n",
    "        truth = reward + (1 - done) * gamma * argmax(model.predict(next_states))\n",
    "        target = model.predict(state)\n",
    "        target[action] = truth\n",
    "    model.fit(targetS) # Array of all computed target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, state_size, action_size):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.99    # discount rate\n",
    "        self.exploration_rate = 1.  # exploration rate\n",
    "        self.original_epsilon = 1.\n",
    "        self.min_epsilon = 0.01\n",
    "        self.exploration_rate_decay = 400\n",
    "        self.n_episodes = 2500 * 200\n",
    "        self.model = self._build_model()\n",
    "        self.target = self._build_model()\n",
    "        self.model.summary()\n",
    "        np.random.seed(42)\n",
    "    \n",
    "    def _build_model(self):\n",
    "       # TODO\n",
    "   \n",
    "    # Memory replay \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Epsilon greedy strategy\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.exploration_rate:\n",
    "            return np.random.randint(self.action_size)\n",
    "        # TODO\n",
    "        else:\n",
    "            pass\n",
    "      \n",
    "    def replay(self, batch_s):\n",
    "        \n",
    "       # TODO\n",
    "    \n",
    "    def run(self):\n",
    "        done = False\n",
    "        batch_size = 32\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, self.state_size))\n",
    "        self.cumulative_reward = 0\n",
    "        score = []\n",
    "        epsilon_logger = []\n",
    "        score_average = []\n",
    "        cpt_episode = 0\n",
    "\n",
    "        for timer in range(self.n_episodes):\n",
    "\n",
    "            action = self.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, (1, self.state_size))\n",
    "            self.remember(state, action, reward, next_state, done)\n",
    "            epsilon_logger.append(self.exploration_rate)\n",
    "            self.cumulative_reward += reward\n",
    "            state = next_state\n",
    "            if done :\n",
    "                state = env.reset()\n",
    "                score_average.append(np.mean(score[-10:]))\n",
    "                state = np.reshape(state, (1, self.state_size))\n",
    "                cpt_episode += 1\n",
    "                score.append(self.cumulative_reward)\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2} \"\n",
    "                          .format(cpt_episode, self.n_episodes, self.cumulative_reward, self.exploration_rate))\n",
    "                self.cumulative_reward = 0\n",
    "                loss = self.replay(batch_size)\n",
    "                self.exploration_rate = max(self.exploration_rate - (self.original_epsilon / self.exploration_rate_decay),\\\n",
    "                                                                self.min_epsilon)\n",
    "                self.target.set_weights(self.model.get_weights())\n",
    "            if timer % 1000 == 0:\n",
    "                \n",
    "                clear_output(True)\n",
    "                self.model.summary()\n",
    "                plt.subplot(311)\n",
    "                axis = plt.gca()\n",
    "                axis.set_ylim([0, 200])\n",
    "                plt.plot(score, label = \"Cumulative reward \")\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.show()\n",
    "                plt.subplot(312)\n",
    "                \n",
    "                axis = plt.gca()\n",
    "                axis.set_ylim([0, 200])\n",
    "                plt.plot(score_average, label = \"Cumulative Mean reward \")\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.show()\n",
    "                plt.subplot(313)\n",
    "                \n",
    "                axis = plt.gca()\n",
    "                axis.set_ylim([0, 1])\n",
    "                plt.plot(epsilon_logger, label =\"Epsilon\")\n",
    "                plt.grid()\n",
    "                plt.legend()\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load Solution/Lab1_exo4_solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "agent_vanilla = DQNAgent(env, state_size, action_size)\n",
    "agent_vanilla.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have problem : relaunch the notebook with this command :\n",
    "# xvfb-run -s \"-screen 0 1400x900x24\" jupyter-notebook \n",
    "model = agent_vanilla.model\n",
    "state = env.reset()\n",
    "state = np.reshape(state, (1, state_size))\n",
    "for i in range(2000):\n",
    "\n",
    "    clear_output(True)\n",
    "    action = np.argmax(model.predict(state)[0])\n",
    "    next_st, reward, done, _= env.step(action)\n",
    "    state = next_st\n",
    "    if done :\n",
    "        state = env.reset()\n",
    "    state = np.reshape(state, (1, state_size))\n",
    "\n",
    "    plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "    env.close()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
